{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7166601,"sourceType":"datasetVersion","datasetId":4107330}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Import The Libraries**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport yaml\nfrom PIL import Image\nfrom ultralytics import YOLO\nfrom IPython.display import Video\nimport numpy as np \nimport pandas as pd \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model1 = YOLO(\"yolov8n.pt\")  \nmodel2 = YOLO(\"yolov8s.pt\")\nmodel3 = YOLO(\"yolov8m.pt\")\nmodel4 = YOLO(\"yolov8l.pt\")\nmodel5 = YOLO(\"yolov8x.pt\")\nimage_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\nresults1 = model1(image_path, show=True)  \nresults2 = model2(image_path, show=True)  \nresults3 = model3(image_path, show=True)  \nresults4 = model4(image_path, show=True)  \nresults5 = model5(image_path, show=True)  \n\nresults1[0].save(\"output1.jpg\")\nresults2[0].save(\"output2.jpg\")\nresults3[0].save(\"output3.jpg\")\nresults4[0].save(\"output4.jpg\")\nresults5[0].save(\"output5.jpg\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img1 = Image.open(\"output1.jpg\")\nimg2 = Image.open(\"output2.jpg\")\nimg3 = Image.open(\"output3.jpg\")\nimg4 = Image.open(\"output4.jpg\")\nimg5 = Image.open(\"output5.jpg\")\n\n\n\nfig, axes = plt.subplots(5, 1, figsize=(15,15)) \nimages = [img1, img2, img3, img4, img5]\n\nfor ax, img in zip(axes, images):\n    ax.imshow(img)\n    ax.axis(\"off\")  \n\nplt.tight_layout() \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **So the best model is YOLOv8n.It is working well on the image with good accuracy and precision.**","metadata":{}},{"cell_type":"code","source":"model = YOLO(\"yolov8s.pt\") \n\ninput_video_path =\"/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4\"\n\noutput_video_path = \"output_video.mp4\"\n\ncap = cv2.VideoCapture(input_video_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video file.\")\n    exit()\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height= int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps         = int(cap.get(cv2.CAP_PROP_FPS))\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  \nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    results = model(frame, conf=0.65)  # Confidence threshold = 0.65\n\n    annotated_frame = results[0].plot()\n\n    out.write(annotated_frame)\n\ncap.release()\nout.release()\n\nprint(f\"Processed video saved as {output_video_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model6=YOLO('yolov8n.pt')\nimage_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n\nresults = model6.predict(source=image_path, imgsz=640, conf=0.5)   # Confidence threshold: 50% \n\nsample_image = results[0].plot(line_width=2)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20,15))\nplt.imshow(sample_image)\nplt.title('Detected Objects in Sample Image by the Pre-trained YOLOv8 Model on COCO Dataset', fontsize=15)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dataset Exploration**","metadata":{}},{"cell_type":"code","source":"dataset_path=('/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset')\nyaml_file_path=os.path.join(dataset_path,'data.yaml')\nwith open(yaml_file_path,'r') as file:\n    yaml_content=yaml.load(file,Loader=yaml.FullLoader)\n    print(yaml.dump(yaml_content, default_flow_style=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_image_path=os.path.join(dataset_path,'train','images')\nvalid_image_path=os.path.join(dataset_path,'valid','images')\n\nnum_train_image=0\nnum_valid_image=0\n\ntrain_image_sizes = set()\nvalid_image_sizes = set()\n\nfor filename in os.listdir(train_image_path):\n    if filename.endswith('.jpg'):\n        num_train_image+=1\n        image_path=os.path.join(train_image_path,filename)\n    with Image.open(image_path) as img:\n        train_image_sizes.add(img.size)\n\nfor filename in os.listdir(valid_image_path):\n    if filename.endswith('.jpg'):\n        num_valid_image+=1\n        image_path=os.path.join(valid_image_path,filename)\n    with Image.open(image_path) as img:\n        valid_image_sizes.add(img.size)\n\n\nprint(f'Number of Training Images is {num_train_image}')\nprint(f'Number of Validation Images is {num_valid_image}')\n\nif len(train_image_sizes)==1:\n    print(f'All Traing Image Has Same Size:{train_image_sizes.pop()}')\nelse:\n    print('All Images are varying images sizes')\n\nif len(valid_image_sizes)==1:\n    print(f'All Validation Image Has Same Size:{valid_image_sizes.pop()}')\nelse:\n    print('All Images are varying images sizes')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_files=[file for file in os.listdir(train_image_path) if file.endswith('.jpg')]\nselected_images=[image_files[i] for i in range(0,len(image_files),len(image_files) // 12)]\n\nfig,axes=plt.subplots(3,4,figsize=(20,16))\nfor ax,img_file in zip(axes.ravel(),selected_images):\n    image_path=os.path.join(train_image_path,img_file)\n    image=Image.open(image_path)\n    ax.imshow(image)\n    ax.axis('off')\n\nplt.suptitle('Sample image from training set',fontsize=30)\nplt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Fine Model Tuning** \n","metadata":{}},{"cell_type":"code","source":"pip install tensorboard\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result=model.train(\n    data=yaml_file_path,\n    epochs=100,\n    imgsz=640,\n    device=0,\n    patience=0,\n    batch=32,\n    optimizer='auto',\n    lr0=0.001,\n    lrf=0.1,\n    dropout=0.1,\n    seed=0,\n    project=\"runs/train\",\n    name=\"exp\"\n    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exp_path='/kaggle/working/runs/train/exp'\nimage_files=[file for file in os.listdir(exp_path) if file.endswith('.png')]\n# selected_images=[image_files[i] for i in range(0,len(image_files),len(image_files) // 12)]\n\nfig,axes=plt.subplots(4,2,figsize=(40,40))\nfor ax,img_file in zip(axes.ravel(),image_files):\n    image_path=os.path.join(exp_path,img_file)\n    image=Image.open(image_path)\n    ax.imshow(image)\n    ax.axis('off')\n\nplt.suptitle('Parameter Curve After Model Tuning',fontsize=10)\nplt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exp_path='/kaggle/working/runs/train/exp'\nimage_files=[file for file in os.listdir(exp_path) if file.endswith('.jpg')]\n# selected_images=[image_files[i] for i in range(0,len(image_files),len(image_files) // 12)]\nfig,axes=plt.subplots(4,3,figsize=(40,40))\nfor ax,img_file in zip(axes.ravel(),image_files):\n    image_path=os.path.join(exp_path,img_file)\n    image=Image.open(image_path)\n    ax.imshow(image)\n    ax.axis('off')\n\nplt.suptitle('Vehile Detection After Model Tuning',fontsize=10)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Learning Curve Analysis**","metadata":{}},{"cell_type":"code","source":"def plot_leqarning_curve(df,train_loss_col,valid_loss_col,title):\n    plt.figure(figsize=(20,12))\n    sns.lineplot(data=df,x='epoch',y=train_loss_col,label='Train Loss',color='blue',linestyle='-',linewidth=2)\n    sns.lineplot(data=df,x='epoch',y=valid_loss_col,label='Valid Loss',color='Orange',linestyle='--',linewidth=2)\n    plt.legend()\n    plt.title(title)\n    plt.xlabel('epoch')\n    plt.ylabel('Loss')\n    plt.show()\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_csv_path=os.path.join(exp_path,'results.csv')\ndf=pd.read_csv(results_csv_path)\ndf.columns=df.columns.str.strip()\nplot_leqarning_curve(df,'train/box_loss','val/box_loss','Box Loss Learning Curve')\nplot_leqarning_curve(df,'train/cls_loss','val/cls_loss','Classification Loss Learning Curve')\nplot_leqarning_curve(df,'train/dfl_loss','val/dfl_loss','Distribution Focal Loss Learning Curve')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Confusion Matrix Evaluation**","metadata":{}},{"cell_type":"code","source":"confusion_matric_path=os.path.join(exp_path,'confusion_matrix_normalized.png')\ncm_img=cv2.imread(confusion_matric_path)\ncm_img=cv2.cvtColor(cm_img,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20),dpi=400)\nplt.imshow(cm_img)\nplt.title('Normalizes Confusion Matrix',fontsize=30)\nplt.tight_layout()\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Performance Metrix Assessment**","metadata":{}},{"cell_type":"code","source":"best_model_path=os.path.join(exp_path,'weights/best.pt')\nbest_model=YOLO(best_model_path)\nmetrics=best_model.val(split='val')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics_df=pd.DataFrame.from_dict(metrics.results_dict,orient='index',columns=['Metric Value'])\nmetrics_df.round(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Inference On Validation Set Images**","metadata":{}},{"cell_type":"code","source":"valid_images_path=os.path.join(dataset_path,'valid','images')\nimage_file=[file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\nselected_image=[image_file[i] for i in range(0,len(image_file),len(image_file) // 12)]\n\nfig,axes=plt.subplots(4,3,figsize=(40,40))\nfig.suptitle('Valid Inference Set',fontsize=40)\n\nfor i,ax in enumerate(axes.flatten()):\n    image_path = os.path.join(valid_images_path, selected_image[i])\n    results = best_model.predict(source=image_path, imgsz=640, conf=0.5)\n    annotated_image = results[0].plot(line_width=1)\n    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n    ax.imshow(annotated_image_rgb)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Inference On Unseen Sample Image**","metadata":{}},{"cell_type":"code","source":"sample_image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n\nresults = best_model.predict(source=sample_image_path, imgsz=640, conf=0.7)   # Confidence threshold: 70% \n\nsample_image = results[0].plot(line_width=2)\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20,15))\nplt.imshow(sample_image)\nplt.title('Detected Objects in Sample Image by the Post-trained Best Model on COCO Dataset', fontsize=15)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Inference On Unseen Sample Video**","metadata":{"execution":{"iopub.status.busy":"2025-01-08T05:20:09.397527Z","iopub.execute_input":"2025-01-08T05:20:09.397864Z","iopub.status.idle":"2025-01-08T05:20:09.401472Z","shell.execute_reply.started":"2025-01-08T05:20:09.397832Z","shell.execute_reply":"2025-01-08T05:20:09.400601Z"}}},{"cell_type":"code","source":"dataset_video_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4'\nvideo_path = '/kaggle/working/bsample_video.mp4'\nshutil.copyfile(dataset_video_path, video_path)\nbest_model.predict(source=video_path, save=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/best_sample_video.avi processed_sample_video.mp4\n\nVideo(\"processed_sample_video.mp4\", embed=True, width=960)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Real-Time Traffic Density Estimation With YOLO**","metadata":{}},{"cell_type":"code","source":"# Define the threshold for considering traffic as heavy\nheavy_traffic_threshold = 10\n\n# Define the vertices for the quadrilaterals\nvertices1 = np.array([(465, 350), (609, 350), (510, 630), (2, 630)], dtype=np.int32)\nvertices2 = np.array([(678, 350), (815, 350), (1203, 630), (743, 630)], dtype=np.int32)\n\n# Define the vertical range for the slice and lane threshold\nx1, x2 = 325, 635 \nlane_threshold = 609\n\n# Define the positions for the text annotations on the image\ntext_position_left_lane = (10, 50)\ntext_position_right_lane = (820, 50)\nintensity_position_left_lane = (10, 100)\nintensity_position_right_lane = (820, 100)\n\n# Define font, scale, and colors for the annotations\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfont_scale = 1\nfont_color = (255, 255, 255)    # White color for text\nbackground_color = (0,255,0)  # Green background for text\n        \n# Open the video\ncap = cv2.VideoCapture('best_sample_video.mp4')\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('traffic_density_analysis.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n# Read until video is completed\nwhile cap.isOpened():\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n    if ret:\n        # Create a copy of the original frame to modify\n        detection_frame = frame.copy()\n    \n        # Black out the regions outside the specified vertical range\n        detection_frame[:x1, :] = 0  # Black out from top to x1\n        detection_frame[x2:, :] = 0  # Black out from x2 to the bottom of the frame\n        \n        # Perform inference on the modified frame\n        results = best_model.predict(detection_frame, imgsz=640, conf=0.4)\n        processed_frame = results[0].plot(line_width=1)\n        \n        # Restore the original top and bottom parts of the frame\n        processed_frame[:x1, :] = frame[:x1, :].copy()\n        processed_frame[x2:, :] = frame[x2:, :].copy()        \n        \n        # Draw the quadrilaterals on the processed frame\n        cv2.polylines(processed_frame, [vertices1], isClosed=True, color=(0, 255, 0), thickness=2)\n        cv2.polylines(processed_frame, [vertices2], isClosed=True, color=(255, 0, 0), thickness=2)\n        \n        # Retrieve the bounding boxes from the results\n        bounding_boxes = results[0].boxes\n\n        # Initialize counters for vehicles in each lane\n        vehicles_in_left_lane = 0\n        vehicles_in_right_lane = 0\n\n        # Loop through each bounding box to count vehicles in each lane\n        for box in bounding_boxes.xyxy:\n            # Check if the vehicle is in the left lane based on the x-coordinate of the bounding box\n            if box[0] < lane_threshold:\n                vehicles_in_left_lane += 1\n            else:\n                vehicles_in_right_lane += 1\n                \n        # Determine the traffic intensity for the left lane\n        traffic_intensity_left = \"Heavy\" if vehicles_in_left_lane > heavy_traffic_threshold else \"Smooth\"\n        # Determine the traffic intensity for the right lane\n        traffic_intensity_right = \"Heavy\" if vehicles_in_right_lane > heavy_traffic_threshold else \"Smooth\"\n\n\n        # Add a background rectangle for the left lane vehicle count\n        cv2.rectangle(processed_frame, (text_position_left_lane[0]-10, text_position_left_lane[1] - 25), \n                      (text_position_left_lane[0] + 460, text_position_left_lane[1] + 10), background_color, -1)\n\n        # Add the vehicle count text on top of the rectangle for the left lane\n        cv2.putText(processed_frame, f'Vehicles in Left Lane: {vehicles_in_left_lane}', text_position_left_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Add a background rectangle for the left lane traffic intensity\n        cv2.rectangle(processed_frame, (intensity_position_left_lane[0]-10, intensity_position_left_lane[1] - 25), \n                      (intensity_position_left_lane[0] + 460, intensity_position_left_lane[1] + 10), background_color, -1)\n\n        # Add the traffic intensity text on top of the rectangle for the left lane\n        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_left}', intensity_position_left_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Add a background rectangle for the right lane vehicle count\n        cv2.rectangle(processed_frame, (text_position_right_lane[0]-10, text_position_right_lane[1] - 25), \n                      (text_position_right_lane[0] + 460, text_position_right_lane[1] + 10), background_color, -1)\n\n        # Add the vehicle count text on top of the rectangle for the right lane\n        cv2.putText(processed_frame, f'Vehicles in Right Lane: {vehicles_in_right_lane}', text_position_right_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Add a background rectangle for the right lane traffic intensity\n        cv2.rectangle(processed_frame, (intensity_position_right_lane[0]-10, intensity_position_right_lane[1] - 25), \n                      (intensity_position_right_lane[0] + 460, intensity_position_right_lane[1] + 10), background_color, -1)\n\n        # Add the traffic intensity text on top of the rectangle for the right lane\n        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_right}', intensity_position_right_lane, \n                    font, font_scale, font_color, 2, cv2.LINE_AA)\n\n        # Write the processed frame to the output video\n        out.write(processed_frame)\n        \n        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n        cv2.imshow('Real-time Analysis', processed_frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press Q on keyboard to exit the loop\n            break\n    else:\n        break\n\n# Release the video capture and video write objects\ncap.release()\nout.release()\n\n# Close all the frames\n# cv2.destroyAllWindows()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ffmpeg -y -loglevel panic -i /kaggle/working/traffic_density_analysis.avi traffic_density_analysis.mp4\n\nVideo(\"traffic_density_analysis.mp4\", embed=True, width=960)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model.export(format='onnx')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}